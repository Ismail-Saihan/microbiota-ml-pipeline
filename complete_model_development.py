import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import (
    accuracy_score, precision_score, recall_score, f1_score, 
    roc_auc_score, classification_report, confusion_matrix, 
    roc_curve, auc
)
import xgboost as xgb
import lightgbm as lgb
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
import joblib
import warnings
warnings.filterwarnings('ignore')
import os
from datetime import datetime

class GutMicrobiotaModelDevelopment:
    """
    Comprehensive model development pipeline for gut microbiota classification
    including baseline, advanced, and transformer-based models
    """
    
    def __init__(self, data_path):
        """Initialize with enhanced dataset"""
        print("üß¨ GUT MICROBIOTA CLASSIFICATION MODEL DEVELOPMENT")
        print("="*65)
        
        self.data_path = data_path
        self.df = pd.read_csv(data_path)
        self.target_column = 'Current status of microbiota'
        
        # Create directories for outputs
        os.makedirs("model_results", exist_ok=True)
        os.makedirs("model_plots", exist_ok=True)
        
        print(f"‚úÖ Dataset loaded: {self.df.shape}")
        print(f"üéØ Target classes: {self.df[self.target_column].value_counts().to_dict()}")
        
        # Initialize containers for results
        self.models = {}
        self.results = {}
        self.predictions = {}
        
    def prepare_data(self):
        """Prepare data for modeling"""
        print("\n1. DATA PREPARATION")
        print("-" * 25)
        
        # Separate features and target
        X = self.df.drop([self.target_column], axis=1)
        y = self.df[self.target_column]
        
        # Handle categorical columns
        categorical_cols = X.select_dtypes(include=['object']).columns.tolist()
        
        # Encode categorical variables
        label_encoders = {}
        for col in categorical_cols:
            le = LabelEncoder()
            X[col] = le.fit_transform(X[col].astype(str))
            label_encoders[col] = le
        
        # Encode target variable
        self.target_encoder = LabelEncoder()
        y_encoded = self.target_encoder.fit_transform(y)
        
        # Split data
        X_train, X_temp, y_train, y_temp = train_test_split(
            X, y_encoded, test_size=0.4, random_state=42, stratify=y_encoded
        )
        
        X_val, X_test, y_val, y_test = train_test_split(
            X_temp, y_temp, test_size=0.5, random_state=42, stratify=y_temp
        )
        
        # Scale features
        self.scaler = StandardScaler()
        X_train_scaled = self.scaler.fit_transform(X_train)
        X_val_scaled = self.scaler.transform(X_val)
        X_test_scaled = self.scaler.transform(X_test)
        
        # Store data
        self.X_train, self.X_val, self.X_test = X_train_scaled, X_val_scaled, X_test_scaled
        self.y_train, self.y_val, self.y_test = y_train, y_val, y_test
        self.feature_names = X.columns.tolist()
        
        print(f"‚úÖ Training set: {X_train_scaled.shape}")
        print(f"‚úÖ Validation set: {X_val_scaled.shape}")
        print(f"‚úÖ Test set: {X_test_scaled.shape}")
        print(f"‚úÖ Features: {len(self.feature_names)}")
        print(f"‚úÖ Classes: {len(np.unique(y_encoded))}")
        
        return label_encoders
    
    def train_baseline_models(self):
        """Train baseline models: Logistic Regression, Random Forest, XGBoost"""
        print("\n2. TRAINING BASELINE MODELS")
        print("-" * 35)
        
        # 1. Logistic Regression
        print("üîÑ Training Logistic Regression...")
        lr_model = LogisticRegression(
            max_iter=1000, 
            random_state=42, 
            class_weight='balanced'
        )
        lr_model.fit(self.X_train, self.y_train)
        self.models['Logistic_Regression'] = lr_model
        
        # 2. Random Forest
        print("üîÑ Training Random Forest...")
        rf_model = RandomForestClassifier(
            n_estimators=100,
            max_depth=10,
            random_state=42,
            class_weight='balanced',
            n_jobs=-1
        )
        rf_model.fit(self.X_train, self.y_train)
        self.models['Random_Forest'] = rf_model
        
        # 3. XGBoost
        print("üîÑ Training XGBoost...")
        xgb_model = xgb.XGBClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            random_state=42,
            eval_metric='mlogloss',
            n_jobs=-1
        )
        xgb_model.fit(self.X_train, self.y_train)
        self.models['XGBoost'] = xgb_model
        
        print("‚úÖ Baseline models trained successfully!")
        
    def train_advanced_models(self):
        """Train advanced models: ANN, LightGBM"""
        print("\n3. TRAINING ADVANCED MODELS")
        print("-" * 32)
        
        # 1. Artificial Neural Network
        print("üîÑ Training Neural Network...")
        ann_model = MLPClassifier(
            hidden_layer_sizes=(128, 64, 32),
            activation='relu',
            solver='adam',
            alpha=0.001,
            batch_size=32,
            learning_rate='adaptive',
            max_iter=500,
            random_state=42,
            early_stopping=True,
            validation_fraction=0.1
        )
        ann_model.fit(self.X_train, self.y_train)
        self.models['Neural_Network'] = ann_model
        
        # 2. LightGBM
        print("üîÑ Training LightGBM...")
        lgb_model = lgb.LGBMClassifier(
            n_estimators=100,
            max_depth=6,
            learning_rate=0.1,
            num_leaves=31,
            random_state=42,
            class_weight='balanced',
            n_jobs=-1,
            verbose=-1
        )
        lgb_model.fit(self.X_train, self.y_train)
        self.models['LightGBM'] = lgb_model
        
        print("‚úÖ Advanced models trained successfully!")
    
    def create_tabtransformer(self, input_dim, num_classes, embed_dim=32, num_heads=8, num_layers=6):
        """Create TabTransformer model architecture"""
        
        class TabTransformer(nn.Module):
            def __init__(self, input_dim, num_classes, embed_dim=32, num_heads=8, num_layers=6):
                super(TabTransformer, self).__init__()
                
                # Feature embeddings - each feature gets its own embedding
                self.feature_embeddings = nn.ModuleList([
                    nn.Linear(1, embed_dim) for _ in range(input_dim)
                ])
                
                # Positional encoding
                self.positional_encoding = nn.Parameter(torch.randn(1, input_dim, embed_dim))
                
                # Transformer layers
                encoder_layer = nn.TransformerEncoderLayer(
                    d_model=embed_dim,
                    nhead=num_heads,
                    dim_feedforward=embed_dim * 4,
                    dropout=0.1,
                    activation='relu',
                    batch_first=True
                )
                self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)
                
                # Classification head
                self.classifier = nn.Sequential(
                    nn.LayerNorm(embed_dim),
                    nn.Linear(embed_dim, embed_dim // 2),
                    nn.ReLU(),
                    nn.Dropout(0.1),
                    nn.Linear(embed_dim // 2, num_classes)
                )
                
            def forward(self, x):
                batch_size = x.size(0)
                
                # Create feature-wise embeddings
                feature_embeddings = []
                for i in range(x.size(1)):
                    feature = x[:, i:i+1]  # (batch, 1)
                    embedded = self.feature_embeddings[i](feature)  # (batch, embed_dim)
                    feature_embeddings.append(embedded.unsqueeze(1))  # (batch, 1, embed_dim)
                
                x = torch.cat(feature_embeddings, dim=1)  # (batch, features, embed_dim)
                
                # Add positional encoding
                x = x + self.positional_encoding
                
                # Apply transformer
                x = self.transformer_encoder(x)  # (batch, features, embed_dim)
                
                # Global average pooling
                x = x.mean(dim=1)  # (batch, embed_dim)
                
                # Classification
                x = self.classifier(x)
                
                return x
        
        return TabTransformer(input_dim, num_classes, embed_dim, num_heads, num_layers)
    
    def train_transformer_model(self):
        """Train TabTransformer model"""
        print("\n4. TRAINING TRANSFORMER MODEL")
        print("-" * 35)
        
        # Prepare data for PyTorch
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"üîÑ Using device: {device}")
        
        # Convert to tensors
        X_train_tensor = torch.FloatTensor(self.X_train)
        y_train_tensor = torch.LongTensor(self.y_train)
        X_val_tensor = torch.FloatTensor(self.X_val)
        y_val_tensor = torch.LongTensor(self.y_val)
        
        # Create data loaders
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
        
        # Create model
        input_dim = self.X_train.shape[1]
        num_classes = len(np.unique(self.y_train))
        
        model = self.create_tabtransformer(
            input_dim=input_dim,
            num_classes=num_classes,
            embed_dim=64,
            num_heads=8,
            num_layers=4
        ).to(device)
        
        # Loss and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
        
        # Training loop
        print("üîÑ Training TabTransformer...")
        num_epochs = 50  # Reduced for faster training
        best_val_acc = 0
        patience_counter = 0
        patience = 15
        
        train_losses, val_losses, val_accuracies = [], [], []
        
        for epoch in range(num_epochs):
            # Training
            model.train()
            train_loss = 0
            for batch_x, batch_y in train_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                
                optimizer.zero_grad()
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            # Validation
            model.eval()
            val_loss = 0
            correct = 0
            total = 0
            
            with torch.no_grad():
                for batch_x, batch_y in val_loader:
                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                    
                    outputs = model(batch_x)
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
                    
                    _, predicted = torch.max(outputs.data, 1)
                    total += batch_y.size(0)
                    correct += (predicted == batch_y).sum().item()
            
            val_acc = 100 * correct / total
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            
            train_losses.append(avg_train_loss)
            val_losses.append(avg_val_loss)
            val_accuracies.append(val_acc)
            
            scheduler.step(avg_val_loss)
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                torch.save(model.state_dict(), 'model_results/best_tabtransformer.pth')
                patience_counter = 0
            else:
                patience_counter += 1
            
            if epoch % 5 == 0:
                print(f"Epoch {epoch:3d}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%")
            
            if patience_counter >= patience:
                print(f"Early stopping at epoch {epoch}")
                break
        
        # Load best model
        model.load_state_dict(torch.load('model_results/best_tabtransformer.pth'))
        model.eval()
        
        self.models['TabTransformer'] = model
        self.transformer_device = device
        
        # Save training history
        training_history = {
            'train_losses': train_losses,
            'val_losses': val_losses,
            'val_accuracies': val_accuracies
        }
        np.save('model_results/transformer_training_history.npy', training_history)
        
        print(f"‚úÖ TabTransformer trained successfully! Best Val Acc: {best_val_acc:.2f}%")
    
    def create_lstm_transformer(self, input_dim, num_classes, hidden_dim=64, num_layers=2):
        """Create LSTM-Transformer hybrid model"""
        
        class LSTMTransformer(nn.Module):
            def __init__(self, input_dim, num_classes, hidden_dim=64, num_layers=2):
                super(LSTMTransformer, self).__init__()
                
                # LSTM layers
                self.lstm = nn.LSTM(
                    input_size=1,
                    hidden_size=hidden_dim,
                    num_layers=num_layers,
                    batch_first=True,
                    dropout=0.1 if num_layers > 1 else 0,
                    bidirectional=True
                )
                
                # Transformer layers
                self.transformer_layer = nn.TransformerEncoderLayer(
                    d_model=hidden_dim * 2,  # *2 for bidirectional
                    nhead=8,
                    dim_feedforward=hidden_dim * 4,
                    dropout=0.1,
                    batch_first=True
                )
                self.transformer = nn.TransformerEncoder(self.transformer_layer, num_layers=2)
                
                # Classification head
                self.classifier = nn.Sequential(
                    nn.LayerNorm(hidden_dim * 2),
                    nn.Linear(hidden_dim * 2, hidden_dim),
                    nn.ReLU(),
                    nn.Dropout(0.1),
                    nn.Linear(hidden_dim, num_classes)
                )
                
            def forward(self, x):
                batch_size = x.size(0)
                
                # Reshape for LSTM: (batch, features, 1)
                x = x.unsqueeze(-1)
                
                # LSTM processing
                lstm_out, _ = self.lstm(x)  # (batch, features, hidden_dim * 2)
                
                # Transformer processing
                transformer_out = self.transformer(lstm_out)  # (batch, features, hidden_dim * 2)
                
                # Global average pooling
                pooled = transformer_out.mean(dim=1)  # (batch, hidden_dim * 2)
                
                # Classification
                output = self.classifier(pooled)
                
                return output
        
        return LSTMTransformer(input_dim, num_classes, hidden_dim, num_layers)
    
    def train_lstm_transformer(self):
        """Train LSTM-Transformer hybrid model"""
        print("\n5. TRAINING LSTM-TRANSFORMER MODEL")
        print("-" * 42)
        
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"üîÑ Using device: {device}")
        
        # Convert to tensors
        X_train_tensor = torch.FloatTensor(self.X_train)
        y_train_tensor = torch.LongTensor(self.y_train)
        X_val_tensor = torch.FloatTensor(self.X_val)
        y_val_tensor = torch.LongTensor(self.y_val)
        
        # Create data loaders
        train_dataset = TensorDataset(X_train_tensor, y_train_tensor)
        val_dataset = TensorDataset(X_val_tensor, y_val_tensor)
        
        train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)
        val_loader = DataLoader(val_dataset, batch_size=64, shuffle=False)
        
        # Create model
        input_dim = self.X_train.shape[1]
        num_classes = len(np.unique(self.y_train))
        
        model = self.create_lstm_transformer(
            input_dim=input_dim,
            num_classes=num_classes,
            hidden_dim=64,
            num_layers=2
        ).to(device)
        
        # Loss and optimizer
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-5)
        scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=10, factor=0.5)
        
        # Training loop
        print("üîÑ Training LSTM-Transformer...")
        num_epochs = 30
        best_val_acc = 0
        
        for epoch in range(num_epochs):
            # Training
            model.train()
            train_loss = 0
            for batch_x, batch_y in train_loader:
                batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                
                optimizer.zero_grad()
                outputs = model(batch_x)
                loss = criterion(outputs, batch_y)
                loss.backward()
                optimizer.step()
                
                train_loss += loss.item()
            
            # Validation
            model.eval()
            val_loss = 0
            correct = 0
            total = 0
            
            with torch.no_grad():
                for batch_x, batch_y in val_loader:
                    batch_x, batch_y = batch_x.to(device), batch_y.to(device)
                    
                    outputs = model(batch_x)
                    loss = criterion(outputs, batch_y)
                    val_loss += loss.item()
                    
                    _, predicted = torch.max(outputs.data, 1)
                    total += batch_y.size(0)
                    correct += (predicted == batch_y).sum().item()
            
            val_acc = 100 * correct / total
            avg_train_loss = train_loss / len(train_loader)
            avg_val_loss = val_loss / len(val_loader)
            
            scheduler.step(avg_val_loss)
            
            if val_acc > best_val_acc:
                best_val_acc = val_acc
                torch.save(model.state_dict(), 'model_results/best_lstm_transformer.pth')
            
            if epoch % 5 == 0:
                print(f"Epoch {epoch:3d}: Train Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}, Val Acc: {val_acc:.2f}%")
        
        # Load best model
        model.load_state_dict(torch.load('model_results/best_lstm_transformer.pth'))
        model.eval()
        
        self.models['LSTM_Transformer'] = model
        
        print(f"‚úÖ LSTM-Transformer trained successfully! Best Val Acc: {best_val_acc:.2f}%")
    
    def evaluate_models(self):
        """Evaluate all models on test set"""
        print("\n6. MODEL EVALUATION")
        print("-" * 25)
        
        results = {}
        
        for model_name, model in self.models.items():
            print(f"üîÑ Evaluating {model_name}...")
            
            if 'Transformer' in model_name:
                # Handle PyTorch models
                device = getattr(self, 'transformer_device', torch.device('cpu'))
                model.eval()
                
                with torch.no_grad():
                    X_test_tensor = torch.FloatTensor(self.X_test).to(device)
                    outputs = model(X_test_tensor)
                    
                    if outputs.dim() > 1:
                        y_pred = torch.argmax(outputs, dim=1).cpu().numpy()
                        y_pred_proba = torch.softmax(outputs, dim=1).cpu().numpy()
                    else:
                        y_pred = (outputs > 0.5).float().cpu().numpy()
                        y_pred_proba = torch.sigmoid(outputs).cpu().numpy()
            else:
                # Handle sklearn models
                y_pred = model.predict(self.X_test)
                y_pred_proba = model.predict_proba(self.X_test)
            
            # Calculate metrics
            accuracy = accuracy_score(self.y_test, y_pred)
            precision = precision_score(self.y_test, y_pred, average='weighted')
            recall = recall_score(self.y_test, y_pred, average='weighted')
            f1 = f1_score(self.y_test, y_pred, average='weighted')
            
            # ROC-AUC for multiclass
            try:
                roc_auc = roc_auc_score(self.y_test, y_pred_proba, multi_class='ovr', average='weighted')
            except:
                roc_auc = 0.0
            
            results[model_name] = {
                'Accuracy': accuracy,
                'Precision': precision,
                'Recall': recall,
                'F1_Score': f1,
                'ROC_AUC': roc_auc,
                'Predictions': y_pred,
                'Predictions_Proba': y_pred_proba
            }
            
            print(f"   Accuracy: {accuracy:.4f}")
            print(f"   F1-Score: {f1:.4f}")
            print(f"   ROC-AUC: {roc_auc:.4f}")
        
        self.results = results
        print("‚úÖ Model evaluation completed!")
        
        return results
    
    def create_visualizations(self):
        """Create comprehensive visualizations"""
        print("\n7. CREATING VISUALIZATIONS")
        print("-" * 33)
        
        # 1. Model Performance Comparison
        self.plot_model_comparison()
        
        # 2. Confusion Matrices
        self.plot_confusion_matrices()
        
        # 3. ROC Curves
        self.plot_roc_curves()
        
        # 4. Feature Importance (for tree-based models)
        self.plot_feature_importance()
        
        # 5. Training History (for transformer models)
        self.plot_training_history()
        
        print("‚úÖ All visualizations created!")
    
    def plot_model_comparison(self):
        """Plot model performance comparison"""
        metrics = ['Accuracy', 'Precision', 'Recall', 'F1_Score', 'ROC_AUC']
        models = list(self.results.keys())
        
        fig, ax = plt.subplots(figsize=(12, 8))
        
        x = np.arange(len(models))
        width = 0.15
        
        for i, metric in enumerate(metrics):
            values = [self.results[model][metric] for model in models]
            ax.bar(x + i*width, values, width, label=metric)
        
        ax.set_xlabel('Models', fontweight='bold')
        ax.set_ylabel('Score', fontweight='bold')
        ax.set_title('Model Performance Comparison', fontweight='bold', fontsize=14)
        ax.set_xticks(x + width * 2)
        ax.set_xticklabels(models, rotation=45)
        ax.legend()
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('model_plots/01_model_comparison.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_confusion_matrices(self):
        """Plot confusion matrices for all models"""
        n_models = len(self.models)
        cols = 3
        rows = (n_models + cols - 1) // cols
        
        fig, axes = plt.subplots(rows, cols, figsize=(15, 5*rows))
        if rows == 1:
            axes = axes.reshape(1, -1)
        
        for i, (model_name, results) in enumerate(self.results.items()):
            row, col = i // cols, i % cols
            
            cm = confusion_matrix(self.y_test, results['Predictions'])
            
            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', 
                       xticklabels=self.target_encoder.classes_,
                       yticklabels=self.target_encoder.classes_,
                       ax=axes[row, col])
            
            axes[row, col].set_title(f'{model_name}', fontweight='bold')
            axes[row, col].set_xlabel('Predicted')
            axes[row, col].set_ylabel('Actual')
        
        # Remove empty subplots
        for i in range(n_models, rows * cols):
            row, col = i // cols, i % cols
            axes[row, col].remove()
        
        plt.tight_layout()
        plt.savefig('model_plots/02_confusion_matrices.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_roc_curves(self):
        """Plot ROC curves for all models"""
        fig, ax = plt.subplots(figsize=(10, 8))
        
        colors = plt.cm.Set1(np.linspace(0, 1, len(self.models)))
        
        for i, (model_name, results) in enumerate(self.results.items()):
            if results['ROC_AUC'] > 0:
                # For multiclass, plot micro-average ROC curve
                from sklearn.preprocessing import label_binarize
                from sklearn.metrics import roc_curve, auc
                
                y_test_bin = label_binarize(self.y_test, classes=range(len(self.target_encoder.classes_)))
                y_pred_proba = results['Predictions_Proba']
                
                # Compute micro-average ROC curve and ROC area
                fpr, tpr, _ = roc_curve(y_test_bin.ravel(), y_pred_proba.ravel())
                roc_auc = auc(fpr, tpr)
                
                ax.plot(fpr, tpr, color=colors[i], lw=2, 
                       label=f'{model_name} (AUC = {roc_auc:.3f})')
        
        ax.plot([0, 1], [0, 1], 'k--', lw=2, label='Random Classifier')
        ax.set_xlim([0.0, 1.0])
        ax.set_ylim([0.0, 1.05])
        ax.set_xlabel('False Positive Rate', fontweight='bold')
        ax.set_ylabel('True Positive Rate', fontweight='bold')
        ax.set_title('ROC Curves - Model Comparison', fontweight='bold', fontsize=14)
        ax.legend(loc="lower right")
        ax.grid(True, alpha=0.3)
        
        plt.tight_layout()
        plt.savefig('model_plots/03_roc_curves.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_feature_importance(self):
        """Plot feature importance for tree-based models"""
        tree_models = ['Random_Forest', 'XGBoost', 'LightGBM']
        available_models = [m for m in tree_models if m in self.models]
        
        if not available_models:
            return
        
        fig, axes = plt.subplots(1, len(available_models), figsize=(6*len(available_models), 8))
        if len(available_models) == 1:
            axes = [axes]
        
        for i, model_name in enumerate(available_models):
            model = self.models[model_name]
            
            if hasattr(model, 'feature_importances_'):
                importances = model.feature_importances_
                
                # Get top 15 features
                indices = np.argsort(importances)[::-1][:15]
                
                axes[i].barh(range(len(indices)), importances[indices])
                axes[i].set_yticks(range(len(indices)))
                axes[i].set_yticklabels([self.feature_names[j] for j in indices])
                axes[i].set_xlabel('Importance')
                axes[i].set_title(f'{model_name} Feature Importance', fontweight='bold')
                axes[i].invert_yaxis()
        
        plt.tight_layout()
        plt.savefig('model_plots/04_feature_importance.png', dpi=300, bbox_inches='tight')
        plt.close()
    
    def plot_training_history(self):
        """Plot training history for transformer models"""
        try:
            history = np.load('model_results/transformer_training_history.npy', allow_pickle=True).item()
            
            fig, axes = plt.subplots(1, 2, figsize=(15, 6))
            
            # Loss curves
            axes[0].plot(history['train_losses'], label='Training Loss', linewidth=2)
            axes[0].plot(history['val_losses'], label='Validation Loss', linewidth=2)
            axes[0].set_title('Training and Validation Loss', fontweight='bold')
            axes[0].set_xlabel('Epoch')
            axes[0].set_ylabel('Loss')
            axes[0].legend()
            axes[0].grid(True, alpha=0.3)
            
            # Accuracy curve
            axes[1].plot(history['val_accuracies'], label='Validation Accuracy', linewidth=2, color='green')
            axes[1].set_title('Validation Accuracy', fontweight='bold')
            axes[1].set_xlabel('Epoch')
            axes[1].set_ylabel('Accuracy (%)')
            axes[1].legend()
            axes[1].grid(True, alpha=0.3)
            
            plt.tight_layout()
            plt.savefig('model_plots/05_training_history.png', dpi=300, bbox_inches='tight')
            plt.close()
            
        except:
            print("‚ö†Ô∏è Training history not available for plotting")
    
    def generate_evaluation_report(self):
        """Generate comprehensive evaluation report"""
        print("\n8. GENERATING EVALUATION REPORT")
        print("-" * 38)
        
        # Create results DataFrame
        results_df = pd.DataFrame(self.results).T
        results_df = results_df[['Accuracy', 'Precision', 'Recall', 'F1_Score', 'ROC_AUC']]
        
        # Save results
        results_df.to_csv('model_results/model_performance_comparison.csv')
        
        # Generate detailed report
        report_content = f"""# Gut Microbiota Classification - Model Evaluation Report

## Project Overview
- **Dataset**: Advanced Feature Engineered Gut Microbiota Data
- **Features**: {len(self.feature_names)}
- **Samples**: {len(self.df)}
- **Classes**: {len(self.target_encoder.classes_)} ({', '.join(self.target_encoder.classes_)})
- **Test Set Size**: {len(self.y_test)}

## Model Performance Summary

### Performance Metrics Table
"""
        
        # Add performance table
        report_content += "| Model | Accuracy | Precision | Recall | F1-Score | ROC-AUC |\n"
        report_content += "|-------|----------|-----------|---------|----------|----------|\n"
        
        for model_name, metrics in self.results.items():
            report_content += f"| {model_name} | {metrics['Accuracy']:.4f} | {metrics['Precision']:.4f} | {metrics['Recall']:.4f} | {metrics['F1_Score']:.4f} | {metrics['ROC_AUC']:.4f} |\n"
        
        # Find best model
        best_model = max(self.results.items(), key=lambda x: x[1]['F1_Score'])
        
        report_content += f"""
### Best Performing Model: {best_model[0]}
- **Accuracy**: {best_model[1]['Accuracy']:.4f}
- **F1-Score**: {best_model[1]['F1_Score']:.4f}
- **ROC-AUC**: {best_model[1]['ROC_AUC']:.4f}

### Model Categories Comparison

#### Baseline Models
- **Logistic Regression**: Traditional linear classifier
- **Random Forest**: Ensemble tree-based method
- **XGBoost**: Gradient boosting framework

#### Advanced Models  
- **Neural Network**: Multi-layer perceptron with 3 hidden layers
- **LightGBM**: Gradient boosting with leaf-wise tree growth

#### Transformer-Based Models
- **TabTransformer**: Transformer architecture for tabular data
- **LSTM-Transformer**: Hybrid sequential and attention-based model

### Key Findings

1. **Best Overall Performance**: {best_model[0]} achieved the highest F1-score of {best_model[1]['F1_Score']:.4f}

2. **Feature Engineering Impact**: The 92 engineered features provide rich context for classification

3. **Class Imbalance Handling**: Models handle the original "At Risk" class imbalance effectively

4. **Transformer Benefits**: Advanced attention mechanisms capture complex feature interactions

### Model-Specific Insights
"""
        
        # Add specific insights for each model type
        for model_name, metrics in self.results.items():
            if 'Transformer' in model_name:
                report_content += f"\n#### {model_name}\n"
                report_content += f"- Utilizes attention mechanisms for feature interactions\n"
                report_content += f"- F1-Score: {metrics['F1_Score']:.4f}\n"
                report_content += f"- Particularly effective at capturing non-linear relationships\n"
        
        report_content += f"""
### Recommendations

1. **Production Deployment**: Use {best_model[0]} for optimal performance
2. **Feature Importance**: Focus on gut health-specific engineered features
3. **Model Ensemble**: Consider combining top 3 models for improved robustness
4. **Continuous Learning**: Update models with new microbiome research findings

### Technical Details

- **Data Preprocessing**: StandardScaler normalization applied
- **Class Weights**: Balanced to handle class imbalance
- **Cross-Validation**: Stratified splits maintain class distribution
- **Evaluation**: Comprehensive metrics including multiclass ROC-AUC

### Files Generated

- `model_performance_comparison.csv`: Detailed metrics comparison
- `model_plots/`: Visualization files
  - `01_model_comparison.png`: Performance comparison bar chart
  - `02_confusion_matrices.png`: Confusion matrices for all models
  - `03_roc_curves.png`: ROC curves comparison
  - `04_feature_importance.png`: Feature importance analysis
  - `05_training_history.png`: Transformer training curves

---
*Report generated on {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*
"""
        
        # Save report
        with open('model_results/evaluation_report.md', 'w', encoding='utf-8') as f:
            f.write(report_content)
        
        print("‚úÖ Evaluation report generated!")
        print(f"üìä Best model: {best_model[0]} (F1: {best_model[1]['F1_Score']:.4f})")
        
        return results_df
    
    def run_complete_pipeline(self):
        """Run the complete model development and evaluation pipeline"""
        print("\nüöÄ RUNNING COMPLETE MODEL DEVELOPMENT PIPELINE")
        print("="*65)
        
        # Execute all steps
        self.prepare_data()
        self.train_baseline_models()
        self.train_advanced_models()
        self.train_transformer_model()
        self.train_lstm_transformer()
        
        results = self.evaluate_models()
        self.create_visualizations()
        results_df = self.generate_evaluation_report()
        
        print(f"\nüéâ MODEL DEVELOPMENT COMPLETED!")
        print("="*45)
        print(f"‚úÖ {len(self.models)} models trained successfully")
        print(f"‚úÖ Comprehensive evaluation completed")
        print(f"‚úÖ Visualizations and reports generated")
        print(f"üìÅ Results saved in 'model_results/' directory")
        print(f"üìà Plots saved in 'model_plots/' directory")
        
        return results_df, self.models

# Main execution
if __name__ == "__main__":
    # Initialize model development
    model_dev = GutMicrobiotaModelDevelopment("advanced_feature_engineered_data.csv")
    
    # Run complete pipeline
    results_df, trained_models = model_dev.run_complete_pipeline()
    
    print(f"\nüìã FINAL RESULTS SUMMARY:")
    print(results_df.round(4))
